{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "import functools as ft\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.dataframe\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"ml_example\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import NaiveBayesModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ Ôªø</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .Ôªø</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13T12:09:31.188000</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "0     LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU             Julius NM   \n",
       "1     LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A           adam riyati   \n",
       "2     LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8      Evgeny Murashkin   \n",
       "3             z13jhp0bxqncu512g22wvzkasxmvvzjaz04       ElNino Melendez   \n",
       "4             z13fwbwp1oujthgqj04chlngpvzmtt3r3dw                GsMega   \n",
       "...                                           ...                   ...   \n",
       "1951  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1952  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1953  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs         jeffrey jules   \n",
       "1954  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1955  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                            DATE  \\\n",
       "0            2013-11-07T06:20:48   \n",
       "1            2013-11-07T12:37:15   \n",
       "2            2013-11-08T17:34:21   \n",
       "3            2013-11-09T08:28:43   \n",
       "4            2013-11-10T16:05:38   \n",
       "...                          ...   \n",
       "1951  2013-07-13T13:27:39.441000   \n",
       "1952  2013-07-13T13:14:30.021000   \n",
       "1953  2013-07-13T12:09:31.188000   \n",
       "1954  2013-07-13T11:17:52.308000   \n",
       "1955  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                                CONTENT  label  \n",
       "0     Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1     Hey guys check out my new channel and our firs...      1  \n",
       "2                just for test I have to say murdev.com      1  \n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ Ôªø      1  \n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .Ôªø      1  \n",
       "...                                                 ...    ...  \n",
       "1951  I love this song because we sing it at Camp al...      0  \n",
       "1952  I love this song for two reasons: 1.it is abou...      0  \n",
       "1953                                                wow      0  \n",
       "1954                            Shakira u are so wiredo      0  \n",
       "1955                         Shakira is the best dancer      0  \n",
       "\n",
       "[1956 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "psy_df = pd.read_csv('data\\Youtube01-Psy.csv')\n",
    "katy_df = pd.read_csv('data\\Youtube02-KatyPerry.csv')\n",
    "lmfao_df = pd.read_csv('data\\Youtube03-LMFAO.csv')\n",
    "eminem_df = pd.read_csv('data\\Youtube04-Eminem.csv')\n",
    "shakira_df = pd.read_csv('data\\Youtube05-Shakira.csv')\n",
    "\n",
    "dataframes = [psy_df, katy_df, lmfao_df, eminem_df, shakira_df]\n",
    "\n",
    "main_df = ft.reduce(lambda left, right: pd.concat([left, right], axis=0, ignore_index=True), dataframes)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Follow:-\n",
    "1. Remove punctuation\n",
    "2. Remove Stopwords\n",
    "3. Remove emojis\n",
    "4. Lemmatization\n",
    "5. Tokenization\n",
    "6. Scaling\n",
    "7. Implement Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ Ôªø</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .Ôªø</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                CONTENT  label\n",
       "0     Huh, anyway check out this you[tube] channel: ...      1\n",
       "1     Hey guys check out my new channel and our firs...      1\n",
       "2                just for test I have to say murdev.com      1\n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ Ôªø      1\n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .Ôªø      1\n",
       "...                                                 ...    ...\n",
       "1951  I love this song because we sing it at Camp al...      0\n",
       "1952  I love this song for two reasons: 1.it is abou...      0\n",
       "1953                                                wow      0\n",
       "1954                            Shakira u are so wiredo      0\n",
       "1955                         Shakira is the best dancer      0\n",
       "\n",
       "[1956 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis=1, inplace=True)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding single letters to the list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'q', 'r', 'u', 'v', 'w', 'x', 'z']\n"
     ]
    }
   ],
   "source": [
    "alphabet_list = list(string.ascii_lowercase)\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for i in range(len(alphabet_list)):\n",
    "    if (alphabet_list[i] in stop_words):\n",
    "        continue\n",
    "    else:\n",
    "        stop_words.append(alphabet_list[i])\n",
    "        \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuations and Random Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, text):\n",
    "    df[text] = df[text].str.lower()\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r'@[A-Za-z0-9]+', '', elem))              # Removing all @mentions\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r'https?:\\/\\/\\S+', '', elem))             # Removing all links\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r'#', '', elem))                          # Removing the hashtag symbol\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r\"([^0-9A-Za-z \\t])\", \"\", elem))          # Removing all non-alphanumeric symbols\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r\"\\w*\\d\\w*\", \"\", elem))                   # Removing all numbers\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r\"^\\d+\\w|^\\w+\\d+\\w|^\\w+\\d$\", \"\", elem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huh anyway check out this youtube channel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just for test i have to say murdevcom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>me shaking my sexy ass on my channel enjoy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watchvvtarggvgtwq   check this out</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>i love this song because we sing it at camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>i love this song for two reasons  is about afr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                CONTENT  label\n",
       "0            huh anyway check out this youtube channel       1\n",
       "1     hey guys check out my new channel and our firs...      1\n",
       "2                 just for test i have to say murdevcom      1\n",
       "3          me shaking my sexy ass on my channel enjoy        1\n",
       "4                   watchvvtarggvgtwq   check this out       1\n",
       "...                                                 ...    ...\n",
       "1951  i love this song because we sing it at camp al...      0\n",
       "1952  i love this song for two reasons  is about afr...      0\n",
       "1953                                                wow      0\n",
       "1954                            shakira u are so wiredo      0\n",
       "1955                         shakira is the best dancer      0\n",
       "\n",
       "[1956 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = clean_text(main_df, 'CONTENT')\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, text):\n",
    "    \n",
    "    alphabet_list = list(string.ascii_lowercase)\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    for i in range(len(alphabet_list)):\n",
    "        if (alphabet_list[i] in stop_words):\n",
    "            continue\n",
    "        else:\n",
    "            stop_words.append(alphabet_list[i])\n",
    "        \n",
    "    ABC = list((df[text]))\n",
    "    clean_lst = []\n",
    "    for i in range(ABC.__len__()):\n",
    "        result_words = [word for word in ABC[i].split(' ') if word not in stop_words]\n",
    "        str_result = ' '.join(result_words)\n",
    "        clean_lst.append(str_result)\n",
    "        \n",
    "    cleaner_df = pd.DataFrame(clean_lst, columns=['CONTENT'])\n",
    "    \n",
    "    return cleaner_df\n",
    "\n",
    "cleaner_df = remove_stopwords(clean_df, 'CONTENT')\n",
    "cleaner_df.head(60)\n",
    "\n",
    "cleaner_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huh anyway check youtube channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey guys check new channel first vid us  monke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test say murdevcom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shaking sexy ass channel enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watchvvtarggvgtwq   check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>love song sing camp time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>love song two reasons  africa  born beautiful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>shakira wiredo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>shakira best dancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                CONTENT\n",
       "0                     huh anyway check youtube channel \n",
       "1     hey guys check new channel first vid us  monke...\n",
       "2                                    test say murdevcom\n",
       "3                      shaking sexy ass channel enjoy  \n",
       "4                            watchvvtarggvgtwq   check \n",
       "...                                                 ...\n",
       "1951                           love song sing camp time\n",
       "1952  love song two reasons  africa  born beautiful ...\n",
       "1953                                                wow\n",
       "1954                                     shakira wiredo\n",
       "1955                                shakira best dancer\n",
       "\n",
       "[1956 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner_df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "# cleaner_df.to_csv('data/temp_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" def get_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'V': wordnet.VERB,\n",
    "        'N': wordnet.NOUN,\n",
    "        'J': wordnet.ADJ,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "    \n",
    "    return tag_dict.get(tag) \"\"\"\n",
    "\n",
    "def lem_df(df, text):\n",
    "    lst_sent = list(df[text])\n",
    "    lem_lst = []\n",
    "    for i in range(len(lst_sent)):\n",
    "        input = nltk.word_tokenize(lst_sent[i])\n",
    "        result_words = [lemmatizer.lemmatize(word) for word in input]\n",
    "        str_result = ' '.join(result_words)\n",
    "        lem_lst.append(str_result)\n",
    "        \n",
    "    cleanest_df = pd.DataFrame(lem_lst, columns=['CONTENT'])\n",
    "    return cleanest_df\n",
    "\n",
    "lemmatized_df = lem_df(cleaner_df, 'CONTENT')\n",
    "lemmatized_df = lemmatized_df.join(clean_df['label'])\n",
    "lemmatized_df.dropna(inplace=True)\n",
    "lemmatized_df.to_csv('data/full_comment_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(20,[1,9,12,17],[...|\n",
      "|    1|(20,[1,2,3,5,6,7,...|\n",
      "|    1|(20,[2,6,8],[1.30...|\n",
      "|    1|(20,[5,7,9,12,17]...|\n",
      "|    1|(20,[1,8],[1.0256...|\n",
      "|    1|(20,[1,2,3,5,8,11...|\n",
      "|    1|(20,[2,12],[1.304...|\n",
      "|    0|(20,[0,1,2,5,13,1...|\n",
      "|    1|(20,[1,12,14,18],...|\n",
      "|    1|(20,[1,10,12,13,1...|\n",
      "|    1|(20,[2,8],[1.3049...|\n",
      "|    1|(20,[2,3,4,5,6,7,...|\n",
      "|    1|(20,[2,10],[2.609...|\n",
      "|    1|(20,[8,10],[1.166...|\n",
      "|    1|(20,[0,1,2,3,4,6,...|\n",
      "|    0|(20,[3,4,13],[1.2...|\n",
      "|    1|(20,[0,1,2,3,4,5,...|\n",
      "|    0|(20,[1,3,4,11,13,...|\n",
      "|    1|(20,[2,3,12],[1.3...|\n",
      "|    1|(20,[5,13,19],[1....|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(20,[0],[1.139699...|\n",
      "|    0|(20,[0],[2.279398...|\n",
      "|    0|(20,[0,1,2,3,4,5,...|\n",
      "|    0|(20,[0,1,2,3,4,6,...|\n",
      "|    0|(20,[0,1,2,3,4,8,...|\n",
      "|    0|(20,[0,1,2,5,13,1...|\n",
      "|    0|(20,[0,1,3,5,8,11...|\n",
      "|    0|(20,[0,1,3,10,11,...|\n",
      "|    0|(20,[0,1,3,11,14,...|\n",
      "|    0|(20,[0,1,3,12,14,...|\n",
      "|    0|(20,[0,1,3,13],[1...|\n",
      "|    0|(20,[0,1,5,9,13,1...|\n",
      "|    0|(20,[0,1,5,11,13,...|\n",
      "|    0|(20,[0,1,6,9,10,1...|\n",
      "|    0|(20,[0,1,7],[1.13...|\n",
      "|    0|(20,[0,1,7,8,19],...|\n",
      "|    0|(20,[0,1,9,10,13,...|\n",
      "|    0|(20,[0,1,9,19],[2...|\n",
      "|    0|(20,[0,2,7,10,11,...|\n",
      "|    0|(20,[0,2,9,14,18]...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = spark.read.csv('data/comment_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "docs_rdd = docs.rdd\n",
    "tokenize = Tokenizer(inputCol='CONTENT', outputCol='WORDS')\n",
    "words_data = tokenize.transform(docs.dropna())\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"WORDS\", outputCol=\"tf_features\", numFeatures=20)\n",
    "featurized_data = hashingTF.transform(words_data)\n",
    "\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaledData = idfModel.transform(featurized_data)\n",
    "\n",
    "# rescaledData.show()\n",
    "final_data = rescaledData.drop('CONTENT', 'WORDS', 'tf_features')\n",
    "final_data.show()\n",
    "\n",
    "splits = final_data.randomSplit([0.8, 0.2], seed=69)\n",
    "train_data = splits[0]\n",
    "test_data = splits[1]\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|            features|\n",
      "+-----+----------+--------------------+\n",
      "|    0|       0.0|(20,[0],[1.139699...|\n",
      "|    0|       0.0|(20,[0],[2.279398...|\n",
      "|    0|       1.0|(20,[0,1,2,3,4,5,...|\n",
      "|    0|       1.0|(20,[0,1,2,3,4,6,...|\n",
      "|    0|       1.0|(20,[0,1,2,3,4,8,...|\n",
      "|    0|       1.0|(20,[0,1,2,5,13,1...|\n",
      "|    0|       0.0|(20,[0,1,3,5,8,11...|\n",
      "|    0|       0.0|(20,[0,1,3,10,11,...|\n",
      "|    0|       0.0|(20,[0,1,3,11,14,...|\n",
      "|    0|       0.0|(20,[0,1,3,12,14,...|\n",
      "|    0|       0.0|(20,[0,1,3,13],[1...|\n",
      "|    0|       0.0|(20,[0,1,5,9,13,1...|\n",
      "|    0|       0.0|(20,[0,1,5,11,13,...|\n",
      "|    0|       0.0|(20,[0,1,6,9,10,1...|\n",
      "|    0|       1.0|(20,[0,1,7],[1.13...|\n",
      "|    0|       0.0|(20,[0,1,7,8,19],...|\n",
      "|    0|       0.0|(20,[0,1,9,10,13,...|\n",
      "|    0|       0.0|(20,[0,1,9,19],[2...|\n",
      "|    0|       0.0|(20,[0,2,7,10,11,...|\n",
      "|    0|       0.0|(20,[0,2,9,14,18]...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Model Accuracy:  0.7705564316287452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and check\n",
    "nb = NaiveBayes(modelType='multinomial')\n",
    "model = nb.fit(train_data)\n",
    "# model.save('model/nbSpamFilter.model')\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"label\", \"prediction\", \"features\").show()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Model Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey guys  please join me in my fight to help abused mistreated animals all  fund will go to helping pay for vet bills and or helping them find homes i  will place an extra emphasis on helping disabled animals ones otherwise  would just be put to sleep by other animal organizations donate please  roaaaaarrrrrr \n"
     ]
    }
   ],
   "source": [
    "txt = \"#Hey guys! 123 Please join me in my fightüêØüêØüêØ to help abused/mistreated animals! All  fund will go to helping pay for vet bills/and or helping them find homes! I  will place an extra emphasis on helping disabled animals, ones otherwise  would just be put to sleep by other animal organizations. Donate please. http://www.gofundme.com/Angels-n-WingzÔªø ROAAAAARRRRRR Ôªø\"\n",
    "\n",
    "def clear_out(text):\n",
    "    mo = text.lower()\n",
    "    mo = re.sub(r'https?:\\/\\/\\S+', '', mo)               # Removing all links\n",
    "    mo = re.sub(r\"/\", ' ', mo)\n",
    "    mo = re.sub(r'@[A-Za-z0-9]+', '', mo)              # Removing all @mentions\n",
    "    mo = re.sub(r'#', '', mo)                           \n",
    "    mo = re.sub(r\"([^0-9A-Za-z \\t])\", \"\", mo)\n",
    "    mo = re.sub(r\"\\w*\\d\\w*\", \"\", mo)\n",
    "    mo = re.sub(r\"^\\d+\\w|^\\w+\\d+\\w|^\\w+\\d$\", \"\", mo)\n",
    "    mo = mo.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    return mo\n",
    "\n",
    "print(clear_out(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             CONTENT|\n",
      "+--------------------+\n",
      "|#Hey guys! 123 Pl...|\n",
      "+--------------------+\n",
      "\n",
      "None\n",
      "(20,[0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n"
     ]
    }
   ],
   "source": [
    "def tfidf_vectorizer(text):\n",
    "    \n",
    "    data = [{'CONTENT': text}]\n",
    "    docs = spark.createDataFrame(data)\n",
    "    \n",
    "    print(docs.show())\n",
    "    \n",
    "    tokenize = Tokenizer(inputCol='CONTENT', outputCol='WORDS')\n",
    "    words_data = tokenize.transform(docs.dropna())\n",
    "\n",
    "    hashingTF = HashingTF(inputCol=\"WORDS\", outputCol=\"tf_features\", numFeatures=20)\n",
    "    featurized_data = hashingTF.transform(words_data)\n",
    "\n",
    "    idf = IDF(inputCol=\"tf_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurized_data)\n",
    "    rescaledData = idfModel.transform(featurized_data)\n",
    "\n",
    "    # rescaledData.show()\n",
    "    final_data = rescaledData.drop('CONTENT', 'WORDS', 'tf_features')\n",
    "    print(final_data.first()['features'])\n",
    "    \n",
    "tfidf_vectorizer(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,[0,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n"
     ]
    }
   ],
   "source": [
    "from YTWebApp import DataPrep\n",
    "\n",
    "dp = DataPrep.data_prepper()\n",
    "\n",
    "print(dp.clean_data(text=txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 "
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    for i in range(10):\n",
    "        yield (i, i+1)\n",
    "        \n",
    "def test2():\n",
    "    \n",
    "    for i in test():\n",
    "        print(i[0], end=\" \")\n",
    "\n",
    "test2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f73e532cbd269d1026df71fd8a305c34d1349d3ece4a0a5c075e0ca7a6a1c7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
